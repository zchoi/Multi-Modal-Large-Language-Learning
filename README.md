# Multi-modal Large Language Model Collection ðŸ¦•
This is a curated list of Multi-modal Large Language Models (MLLM), Multimodal Benchmarks (MMB), Multimodal Instruction Tuning (MMIT), Multimodal In-context Learning (MMIL), Foundation Models (*e.g.*, CLIP families) (FM), and the most popular Parameter-Efficient Tuning methods.

## ðŸ“’Table of Contents
- [Multi-modal Large Language Models (MLLM)](#multimodal-large-language-models)
- [Multimodal Benchmarks (MMB)](#multimodal-benchmarks)
- [Foundation Models (FM)](#foundation-models)
- [Parameter-Efficient Tuning Repo (PETR)](#parameter-efficient-tuning-repo)

> ### Multi-modal Large Language Models (MLLM)

* **Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models** [Arxiv 2024/02/12] [[Paper](https://arxiv.org/abs/2402.07865)] [[Code](https://github.com/TRI-ML/prismatic-vlms)] [[Evaluation](https://github.com/TRI-ML/vlm-evaluation)]<br>
Stanford, Toyota Research Institute


* **Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models** [Arxiv 2024/03/27] [[Paper](https://arxiv.org/pdf/2403.18814.pdf)] [[Code](https://github.com/dvlab-research/MiniGemini)] [[Project Page](https://mini-gemini.github.io/)]<br>
The Chinese University of Hong Kong, SmartMore


* **InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks** [Arxiv 2024/01/15] [[Paper](https://arxiv.org/abs/2312.14238)] [[Code](https://github.com/OpenGVLab/InternVL)]<br>
OpenGVLab, Shanghai AI Laboratory, Nanjing University, The University of Hong Kong, The Chinese University of Hong Kong, Tsinghua University, University of Science and Technology of China, SenseTime Research

* **GiT: Towards Generalist Vision Transformer through Universal Language Interface** [Arxiv 2024/03/14] [[Paper](https://arxiv.org/abs/2403.09394)]<br>
Peking University, Max Planck Institute for Informatics, The Chinese University of Hong Kong Shenzhen, ETH Zurich, The Chinese University of Hong Kong<br>

* **LLaMA: Open and Efficient Foundation Language Models** [Arxiv 2023] [[Paper](https://arxiv.org/pdf/2302.13971v1.pdf)] [[Github Repo](https://github.com/CHENGY12/PLOT)]<br>
Meta AI


> ### Foundation Models (FM)

> ### Parameter-Efficient Tuning Repo (PETR)
* **PEFT: Parameter-Efficient Fine-Tuning** [HuggingFace ðŸ¤—] [[Home Page](https://huggingface.co/docs/peft/index)] [[Code](https://github.com/huggingface/peft)]<br>
PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. <br>

* **LLaMA Efficient Tuning** [[Github Repo](https://github.com/hiyouga/LLaMA-Efficient-Tuning)]<br>
Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA) (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen). <br>

* **LLaMA-Adapter: Efficient Fine-tuning of LLaMA** ðŸš€[[Code](https://github.com/OpenGVLab/LLaMA-Adapter)]<br>
Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters <br>

* **LLaMA2-Accessory** ðŸš€[[Code](https://github.com/Alpha-VLLM/LLaMA2-Accessory)]<br>
An Open-source Toolkit for LLM Development <be>

* **LLaMA Factory: Training and Evaluating Large Language Models with Minimal Effort** [Code](https://github.com/hiyouga/LLaMA-Factory)]<br>
Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM3)
